{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82d4uj_wsU5i"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from html import unescape"
      ],
      "metadata": {
        "id": "FaZY0Ufng8vi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83jxSNTWVKXM",
        "outputId": "48eff8f8-002e-4c83-beeb-1f98363c3d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O9S3JIPRVMw5"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/My Drive/Colab Notebooks/dblp.xml'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "def parse_dblp(xml_file):\n",
        "    context = ET.iterparse(xml_file, events=('end',))\n",
        "    _, root = next(context)\n",
        "    records = []\n",
        "\n",
        "    for event, elem in context:\n",
        "        if elem.tag in {'article', 'inproceedings', 'proceedings', 'book', 'incollection', 'phdthesis', 'mastersthesis'}:\n",
        "            record = {}\n",
        "            for child in elem:\n",
        "                if child.tag in record:\n",
        "                    if isinstance(record[child.tag], list):\n",
        "                        record[child.tag].append(child.text)\n",
        "                    else:\n",
        "                        record[child.tag] = [record[child.tag], child.text]\n",
        "                else:\n",
        "                    record[child.tag] = child.text\n",
        "            records.append(record)\n",
        "            elem.clear()\n",
        "            root.clear()\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Parse the XML file\n",
        "df = parse_dblp(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "sm7iL0_xjGSw",
        "outputId": "f801eeb0-f78e-49e2-8b89-38c11552b554"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParseError",
          "evalue": "undefined entity &Ouml;: line 90, column 17 (<string>)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-4-3b2c40278350>\"\u001b[0m, line \u001b[1;32m26\u001b[0m, in \u001b[1;35m<cell line: 0>\u001b[0m\n    df = parse_dblp(file_path)\n",
            "  File \u001b[1;32m\"<ipython-input-4-3b2c40278350>\"\u001b[0m, line \u001b[1;32m9\u001b[0m, in \u001b[1;35mparse_dblp\u001b[0m\n    for event, elem in context:\n",
            "  File \u001b[1;32m\"/usr/lib/python3.11/xml/etree/ElementTree.py\"\u001b[0m, line \u001b[1;32m1251\u001b[0m, in \u001b[1;35miterator\u001b[0m\n    yield from pullparser.read_events()\n",
            "  File \u001b[1;32m\"/usr/lib/python3.11/xml/etree/ElementTree.py\"\u001b[0m, line \u001b[1;32m1327\u001b[0m, in \u001b[1;35mread_events\u001b[0m\n    raise event\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.11/xml/etree/ElementTree.py\"\u001b[0;36m, line \u001b[0;32m1299\u001b[0;36m, in \u001b[0;35mfeed\u001b[0;36m\u001b[0m\n\u001b[0;31m    self._parser.feed(data)\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m undefined entity &Ouml;: line 90, column 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    xml_content = file.read()\n",
        "\n",
        "# Unescape HTML entities like &Ouml;, &amp;, etc.\n",
        "xml_content = unescape(xml_content)\n",
        "\n",
        "# Save the cleaned XML content back to a new file (this helps to avoid reading the same file again)\n",
        "cleaned_file_path = 'cleaned_dblp.xml'\n",
        "with open(cleaned_file_path, 'w', encoding='utf-8') as cleaned_file:\n",
        "    cleaned_file.write(xml_content)\n",
        "\n",
        "# Now use iterparse with the cleaned file to avoid memory issues\n",
        "context = ET.iterparse(cleaned_file_path, events=('start', 'end'))\n",
        "_, root = next(context)  # Grab the root element to start the parsing\n",
        "\n",
        "papers = []\n",
        "\n",
        "# Define tags of interest\n",
        "valid_tags = ['article', 'inproceedings', 'proceedings', 'book', 'incollection', 'phdthesis', 'mastersthesis']\n",
        "\n",
        "# Process the XML in a memory-efficient manner\n",
        "for event, elem in context:\n",
        "    if event == 'end' and elem.tag in valid_tags:\n",
        "        # Extract relevant data\n",
        "        entry_type = elem.tag\n",
        "        title = elem.find('title').text if elem.find('title') is not None else ''\n",
        "        authors = [author.text for author in elem.findall('.//author')]  # List of authors\n",
        "        keywords = [keyword.text for keyword in elem.findall('.//keyword')]  # List of keywords\n",
        "        citations = elem.find('citation').text if elem.find('citation') is not None else 0\n",
        "\n",
        "        # Append data to the list\n",
        "        papers.append({\n",
        "            'type': entry_type,\n",
        "            'title': title,\n",
        "            'authors': authors,\n",
        "            'keywords': keywords,\n",
        "            'citations': citations\n",
        "        })\n",
        "\n",
        "        # Clear the element to free up memory\n",
        "        root.clear()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "# Print the first few rows to verify\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "9eobc9R7gt66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from html import unescape\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path to your XML file\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/dblp.xml'\n",
        "\n",
        "\n",
        "# Function to unescape HTML entities in chunks (avoiding large memory usage)\n",
        "def unescape_xml_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    return unescape(content)\n",
        "\n",
        "# Process the XML file in chunks\n",
        "def parse_xml_in_chunks(file_path):\n",
        "    context = ET.iterparse(file_path, events=('start', 'end'))\n",
        "    _, root = next(context)  # Grab the root element to start the parsing\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    valid_tags = ['article', 'inproceedings', 'proceedings', 'book', 'incollection', 'phdthesis', 'mastersthesis']\n",
        "\n",
        "    # Processing in chunks\n",
        "    for event, elem in context:\n",
        "        if event == 'end' and elem.tag in valid_tags:\n",
        "            # Extract relevant data\n",
        "            entry_type = elem.tag\n",
        "            title = elem.find('title').text if elem.find('title') is not None else ''\n",
        "            authors = [author.text for author in elem.findall('.//author')]  # List of authors\n",
        "            keywords = [keyword.text for keyword in elem.findall('.//keyword')]  # List of keywords\n",
        "            citations = elem.find('citation').text if elem.find('citation') is not None else 0\n",
        "\n",
        "            # Append data to the list\n",
        "            papers.append({\n",
        "                'type': entry_type,\n",
        "                'title': title,\n",
        "                'authors': authors,\n",
        "                'keywords': keywords,\n",
        "                'citations': citations\n",
        "            })\n",
        "\n",
        "            # Clear the element to free up memory\n",
        "            root.clear()\n",
        "\n",
        "        # To avoid too much memory usage, you could process in batches\n",
        "        if len(papers) > 1000:  # Adjust this to your needs\n",
        "            # Convert to DataFrame and process, then clear the data\n",
        "            df = pd.DataFrame(papers)\n",
        "            # Optionally, save or process the chunk here\n",
        "            # For now, we clear the list to save memory\n",
        "            papers.clear()\n",
        "\n",
        "    # Final conversion of any remaining data\n",
        "    if papers:\n",
        "        df = pd.DataFrame(papers)\n",
        "        # Optionally, save the final DataFrame\n",
        "        papers.clear()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 1: Unescape XML file content to handle special characters\n",
        "xml_content = unescape_xml_file(file_path)\n",
        "\n",
        "# Save the cleaned XML content to a temporary file\n",
        "cleaned_file_path = 'cleaned_dblp.xml'\n",
        "with open(cleaned_file_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(xml_content)\n",
        "\n",
        "# Step 2: Parse the cleaned XML file in chunks\n",
        "df = parse_xml_in_chunks(cleaned_file_path)\n",
        "\n",
        "# Check the result\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "V9kXUS2ihuhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZOkNmL6hufS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ysKth29xhuct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3R9EZ4vnhuX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oWW6c9oUMCG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Combine titles and keywords for vectorization\n",
        "df['text'] = df['title'] + \" \" + df['keywords'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_scaled = scaler.fit_transform(X.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZmlYMH3UL_A"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Compute the Euclidean distance matrix\n",
        "distance_matrix = pairwise_distances(X_scaled, metric='euclidean')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuRCMPTuUL37"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as sch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Perform Agglomerative Clustering\n",
        "Z = sch.linkage(distance_matrix, method='ward')  # Using 'ward' minimizes the variance of clusters\n",
        "\n",
        "# Plot the Dendrogram\n",
        "plt.figure(figsize=(10, 7))\n",
        "sch.dendrogram(Z)\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Paper Index')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBLV5foEUL0X"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "# Cutting the dendrogram to form flat clusters\n",
        "max_d = 50  # This is a distance threshold; you can change it based on your dendrogram\n",
        "clusters = fcluster(Z, max_d, criterion='distance')\n",
        "\n",
        "# Add cluster labels to the dataframe\n",
        "df['cluster'] = clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FShsdL8jdP5s"
      },
      "outputs": [],
      "source": [
        "import lxml.etree as ET\n",
        "\n",
        "# Parse using lxml\n",
        "tree = ET.parse(file_path)\n",
        "root = tree.getroot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yERKaGRULxg"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Perform PCA for dimensionality reduction to 2D\n",
        "pca = PCA(n_components=2)\n",
        "pca_components = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Scatter plot of the clusters\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(pca_components[:, 0], pca_components[:, 1], c=df['cluster'], cmap='rainbow')\n",
        "plt.title('Cluster Visualization (PCA)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwRq7NHuULt0"
      },
      "outputs": [],
      "source": [
        "# Analyze clusters\n",
        "for cluster_num in df['cluster'].unique():\n",
        "    cluster_papers = df[df['cluster'] == cluster_num]\n",
        "    print(f\"Cluster {cluster_num} contains {len(cluster_papers)} papers\")\n",
        "    print(f\"Most common keywords in Cluster {cluster_num}:\")\n",
        "    all_keywords = ' '.join(cluster_papers['keywords'].apply(lambda x: ' '.join(x)))\n",
        "    print(pd.Series(all_keywords.split()).value_counts().head(10))\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSz0cuatULqO"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8cA475GULm6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A1KlGBOULj2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC8JhwUnULgd"
      },
      "outputs": [],
      "source": [
        "https://www.kaggle.com/datasets/dheerajmpai/dblp2023 ,this is the dataset and  ask-2. Hierarchical Clustering\n",
        "\n",
        "Concept:\n",
        "\n",
        "Hierarchical clustering creates a tree-like structure (dendrogram) representing nested clusters.\n",
        "\n",
        "It does not require predefining K.\n",
        "\n",
        "Two types:\n",
        "\n",
        "Agglomerative (Bottom-Up): Each data point starts as its own cluster, and clusters are merged iteratively.\n",
        "\n",
        "Divisive (Top-Down): Starts with one large cluster, which is split recursively.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Normalize the dataset.\n",
        "\n",
        "Compute the distance matrix using Euclidean distance.\n",
        "\n",
        "Apply Agglomerative Clustering (most common method).\n",
        "\n",
        "Use the Dendrogram to determine the optimal number of clusters.\n",
        "\n",
        "Visualize your analysis before and after clustering\n",
        "Analyze and interpret cluster characteristics.\n",
        "For example : Network Feature Extraction: Construct adjacency matrices from:\n",
        "\n",
        "Citation relationships\n",
        "Author-venue affiliations\n",
        "Paper keyword co-occurrence\n",
        "Cluster Interpretation Framework\n",
        "Provide domain-specific analysis prompts: \"Identify emerging CS subfields through cluster evolution 2010-2025\" \"Map cluster hierarchies to ACM Computing Classification System\" \"Analyze Nobel laureate collaboration patterns through dendrogram cuts\"\n",
        "\n",
        "Dataset for Hierarchical Clustering- The DBLP Computer Science Bibliography Dataset\n",
        "\n",
        "\n",
        "[ ]\n",
        "1  this is te task .. do sth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4vcuIrdULdF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTb2dEVpULZf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34YJqTtVULXD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqp36L2tULT-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6qCKsaAULRL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LepptVsJUMEt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example: Extracting paper titles, authors, keywords, and citations\n",
        "papers = []\n",
        "for entry in root.findall(\".//article\"):  # Adjust this path based on the XML structure\n",
        "    title = entry.find('title').text if entry.find('title') is not None else ''\n",
        "    authors = [author.text for author in entry.findall('.//author')]  # List of authors\n",
        "    keywords = [keyword.text for keyword in entry.findall('.//keyword')]  # List of keywords\n",
        "    citations = entry.find('citation').text if entry.find('citation') is not None else 0\n",
        "    papers.append({'title': title, 'authors': authors, 'keywords': keywords, 'citations': citations})\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXiHYAdjULOx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6KY2yy3ULLq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9wWch4RULJR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiACH_mzULF6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm28hzTXULDn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23hAZ8QlULAq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI2pkYgCUK-L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxXZEuAoUK7v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCQIxdjrUK5M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqbKQM9BUK2p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL-09smHUK0N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BZlnWmEUKxz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDVDUpP7UKvn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIMnJoKMUKtn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6vkpeWmUKq5"
      },
      "outputs": [],
      "source": [
        "# # Load the XML file\n",
        "# tree = ET.parse(file_path)  # Replace with the path to your dataset file\n",
        "# root = tree.getroot()\n",
        "\n",
        "# # Inspect the structure of the XML\n",
        "# print(root.tag, root.attrib)\n",
        "\n",
        "# import xml.etree.ElementTree as ET\n",
        "# from html import unescape  # Import unescape to handle special characters\n",
        "\n",
        "# # Read the XML file and unescape entities\n",
        "# with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#     xml_content = file.read()\n",
        "\n",
        "# # Unescape HTML entities (like &Ouml;)\n",
        "# xml_content = unescape(xml_content)\n",
        "\n",
        "# # Now parse the cleaned XML content\n",
        "# root = ET.fromstring(xml_content)\n",
        "\n",
        "# # Proceed with your existing parsing code\n",
        "# context = ET.iterparse(file_path, events=('start', 'end'))\n",
        "# _, root = next(context)\n",
        "\n",
        "# # Continue with your logic...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD-qbsWriTvv"
      },
      "outputs": [],
      "source": [
        "# Path to the XML file (update with your path)\n",
        "file_path = \"dblp.xml\"\n",
        "\n",
        "# Parse the XML\n",
        "tree = ET.parse(file_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "# Extract data\n",
        "entries = []\n",
        "for elem in root.findall('article'):\n",
        "    title = elem.find('title').text if elem.find('title') is not None else None\n",
        "    year = elem.find('year').text if elem.find('year') is not None else None\n",
        "    journal = elem.find('journal').text if elem.find('journal') is not None else None\n",
        "    authors = [author.text for author in elem.findall('author')]\n",
        "\n",
        "    entries.append({\n",
        "        'title': title,\n",
        "        'year': year,\n",
        "        'journal': journal,\n",
        "        'authors': \", \".join(authors)\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(entries)\n",
        "\n",
        "# Drop entries with missing data\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Preview\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "collapsed": true,
        "id": "NOOskf1-biS-",
        "outputId": "53d6b7ff-f3d1-4dd9-a097-386d3090cdf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing XML (this may take a few minutes)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "166it [00:00, 81624.20it/s]\n"
          ]
        },
        {
          "ename": "ParseError",
          "evalue": "undefined entity &Ouml;: line 90, column 17 (<string>)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-16-946d3b177f64>\"\u001b[0m, line \u001b[1;32m13\u001b[0m, in \u001b[1;35m<cell line: 0>\u001b[0m\n    for event, elem in tqdm(context):\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\"\u001b[0m, line \u001b[1;32m1181\u001b[0m, in \u001b[1;35m__iter__\u001b[0m\n    for obj in iterable:\n",
            "  File \u001b[1;32m\"/usr/lib/python3.11/xml/etree/ElementTree.py\"\u001b[0m, line \u001b[1;32m1251\u001b[0m, in \u001b[1;35miterator\u001b[0m\n    yield from pullparser.read_events()\n",
            "  File \u001b[1;32m\"/usr/lib/python3.11/xml/etree/ElementTree.py\"\u001b[0m, line \u001b[1;32m1327\u001b[0m, in \u001b[1;35mread_events\u001b[0m\n    raise event\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.11/xml/etree/ElementTree.py\"\u001b[0;36m, line \u001b[0;32m1299\u001b[0;36m, in \u001b[0;35mfeed\u001b[0;36m\u001b[0m\n\u001b[0;31m    self._parser.feed(data)\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m undefined entity &Ouml;: line 90, column 17\n"
          ]
        }
      ],
      "source": [
        "# Define tags of interestc\n",
        "valid_tags = ['article', 'inproceedings', 'proceedings', 'book', 'incollection', 'phdthesis', 'mastersthesis']\n",
        "\n",
        "# Parse in an efficient way\n",
        "context = ET.iterparse(file_path, events=('start', 'end'))\n",
        "_, root = next(context)  # grab the root element\n",
        "\n",
        "# For storing parsed data\n",
        "data = []\n",
        "\n",
        "print(\"Parsing XML (this may take a few minutes)...\")\n",
        "\n",
        "for event, elem in tqdm(context):\n",
        "    if event == 'end' and elem.tag in valid_tags:\n",
        "        entry_type = elem.tag\n",
        "        authors = [a.text for a in elem.findall('author')]\n",
        "        year_elem = elem.find('year')\n",
        "        venue_elem = elem.find('journal') or elem.find('booktitle')\n",
        "\n",
        "        if authors and year_elem is not None:\n",
        "            year = year_elem.text\n",
        "            venue = venue_elem.text if venue_elem is not None else 'Unknown'\n",
        "            data.append({\n",
        "                'type': entry_type,\n",
        "                'author_count': len(authors),\n",
        "                'year': int(year),\n",
        "                'venue': venue\n",
        "            })\n",
        "\n",
        "        # Clear the element to save memory\n",
        "        root.clear()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Parsed entries:\", len(df))\n",
        "df.head()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}